{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ba7db3f8-ad77-4b60-a64e-237209b0e836",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "# Faster Sentiment Analysis\n",
    "\n",
    "In this notebook, we'll implement a model that gets comparable results whilst training significantly faster and using around half of the parameters. More specifically, we'll be implementing the \"FastText\" model from the paper [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "0b4dfc1b-cfd1-4a51-a8df-7b225cd0b06b",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchtext==0.4.0 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.17.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (4.56.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (4.0.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.1.1)\n",
      "Collecting pip\n",
      "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (56.2.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-59.6.0-py3-none-any.whl (952 kB)\n",
      "\u001b[K     |████████████████████████████████| 952 kB 29.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (0.36.2)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.36.2\n",
      "    Uninstalling wheel-0.36.2:\n",
      "      Successfully uninstalled wheel-0.36.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 56.2.0\n",
      "    Uninstalling setuptools-56.2.0:\n",
      "      Successfully uninstalled setuptools-56.2.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.1\n",
      "    Uninstalling pip-21.1.1:\n",
      "      Successfully uninstalled pip-21.1.1\n",
      "Successfully installed pip-21.3.1 setuptools-59.6.0 wheel-0.37.1\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: spacy==2.2.1 in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (2.25.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (7.1.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (0.9.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.1) (1.17.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.1) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.1) (2020.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.2.0,>=7.1.1->spacy==2.2.1) (4.56.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting en_core_web_sm==2.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0 MB)\n",
      "     |████████████████████████████████| 12.0 MB 22.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.0) (2.2.1)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.25.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.17.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.26.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.2.0,>=7.1.1->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.56.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.1.0\n",
    "!pip install torchtext==0.4.0\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install spacy==2.2.1\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "04c9e5ea-4a42-470b-a705-2e25236821ea",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "One of the key concepts in the FastText paper is that they calculate the n-grams of an input sentence and append them to the end of a sentence. Here, we'll use bi-grams. Briefly, a bi-gram is a pair of words/tokens that appear consecutively within a sentence. \n",
    "\n",
    "For example, in the sentence \"how are you ?\", the bi-grams are: \"how are\", \"are you\" and \"you ?\".\n",
    "\n",
    "The `generate_bigrams` function takes a sentence that has already been tokenized, calculates the bi-grams and appends them to the end of the tokenized list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "e4226eb2-cbbc-4ab6-bbdc-ab66cd0ca4b7",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f9d1807a-8c0c-42a3-a02d-07a7d49486b6",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 3,
     "id": "ab1c7395-d75f-45ad-9ab2-5fd61d91fa93",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'film', 'is', 'terrible', 'film is', 'is terrible', 'This film']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigrams(['This', 'film', 'is', 'terrible'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1640ab25-d383-4dd2-a75f-24e2223a7af9",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "TorchText `Field`s have a `preprocessing` argument. A function passed here will be applied to a sentence after it has been tokenized (transformed from a string into a list of tokens), but before it has been numericalized (transformed from a list of tokens to a list of indexes). This is where we'll pass our `generate_bigrams` function.\n",
    "\n",
    "As we aren't using an RNN we can't use packed padded sequences, thus we do not need to set `include_lengths = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "1ead4d7d-8c19-49d8-b82a-ab3615eedf63",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', preprocessing = generate_bigrams)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "fb308dd2-011f-4f61-810a-bf6c1b616501",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 6,
     "id": "d8eb5851-4b12-424e-8d58-b9a400f0d943",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 7,
     "id": "f42e6a87-eb5d-44b7-8342-e5525cb4591d",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "This model has far fewer parameters than the previous model as it only has 2 layers that have any parameters, the embedding layer and the linear layer. There is no RNN component in sight!\n",
    "\n",
    "Instead, it first calculates the word embedding for each word using the `Embedding` layer, then calculates the average of all of the word embeddings and feeds this through the `Linear` layer, and that's it!\n",
    "\n",
    "We implement the averaging with the `avg_pool2d` (average pool 2-dimensions) function. Initially, you may think using a 2-dimensional pooling seems strange, surely our sentences are 1-dimensional, not 2-dimensional? However, you can think of the word embeddings as a 2-dimensional grid, where the words are along one axis and the dimensions of the word embeddings are along the other.\n",
    "\n",
    "The `avg_pool2d` uses a filter of size `embedded.shape[1]` (i.e. the length of the sentence) by 1.\n",
    "\n",
    "We calculate the average value of all elements covered by the filter, then the filter then slides to the right, calculating the average over the next column of embedding values for each word in the sentence. \n",
    "\n",
    "Each filter position gives us a single value, the average of all covered elements. After the filter has covered all embedding dimensions we get a [1x5] tensor. This tensor is then passed through the linear layer to produce our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 8,
     "id": "bc7f3596-ca8a-49f6-85cb-eefb59aaa90f",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        \n",
    "        #pooled = [batch size, embedding_dim]\n",
    "                \n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, we'll create an instance of our `FastText` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 9,
     "id": "8dceaff7-fc1e-42e6-995b-913ce9f34a1e",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number of parameters in our model, we see we have about the same as the standard RNN from the first notebook and half the parameters of the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 10,
     "id": "4b5892f4-0e3b-4231-93ce-3b7bd452fad0",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,500,301 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And copy the pre-trained vectors to our embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 11,
     "id": "db26c8db-2edc-4adc-b9ef-13e3a5369db5",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-1.3126, -1.6717,  0.4203,  ...,  0.2348, -0.9110,  1.0914],\n",
       "        [-1.5268,  1.5639, -1.0541,  ...,  1.0045, -0.6813, -0.8846],\n",
       "        [-0.7954,  0.7527,  0.3545,  ...,  0.1126, -0.2425,  0.0819]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not forgetting to zero the initial weights of our unknown and padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 12,
     "id": "22aad392-2dcc-4bf3-bb20-6585ec6d7bce",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is the exact same as last time.\n",
    "\n",
    "We initialize our optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 13,
     "id": "1f4ad6c3-fbd1-445f-a6db-2147c88706d2",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 14,
     "id": "9762a5ac-ad4f-49f2-a95e-794e05589017",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 15,
     "id": "93eea34a-970b-4bc1-a0fa-f5920712036c",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 16,
     "id": "74efa374-e6be-4933-b86f-7d62f09fbb82",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 17,
     "id": "040bfaee-a4d5-4846-84a9-b9839e27c700",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "be6be372-5c7d-4638-8666-57913de154a1",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "As before, we'll implement a useful function to tell us how long an epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 18,
     "id": "6bb5d36b-a181-4131-a70d-4db7b0b9c24d",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d51fefda-b11c-4376-99dd-81a93c00bab3",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "Finally, we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": true,
     "execution_count": 19,
     "id": "691439a2-3fcf-4064-9a8b-2de8ecf4c3fa",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.678 | Train Acc: 61.34%\n",
      "\t Val. Loss: 0.575 |  Val. Acc: 73.98%\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.584 | Train Acc: 77.54%\n",
      "\t Val. Loss: 0.434 |  Val. Acc: 80.25%\n",
      "Epoch: 03 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.463 | Train Acc: 84.66%\n",
      "\t Val. Loss: 0.361 |  Val. Acc: 85.09%\n",
      "Epoch: 04 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.378 | Train Acc: 88.05%\n",
      "\t Val. Loss: 0.357 |  Val. Acc: 86.97%\n",
      "Epoch: 05 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.321 | Train Acc: 90.14%\n",
      "\t Val. Loss: 0.366 |  Val. Acc: 87.91%\n",
      "Epoch: 06 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.281 | Train Acc: 91.24%\n",
      "\t Val. Loss: 0.379 |  Val. Acc: 88.63%\n",
      "Epoch: 07 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.252 | Train Acc: 92.09%\n",
      "\t Val. Loss: 0.396 |  Val. Acc: 89.20%\n",
      "Epoch: 08 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.224 | Train Acc: 92.76%\n",
      "\t Val. Loss: 0.416 |  Val. Acc: 89.53%\n",
      "Epoch: 09 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.203 | Train Acc: 93.48%\n",
      "\t Val. Loss: 0.432 |  Val. Acc: 89.63%\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.183 | Train Acc: 94.28%\n",
      "\t Val. Loss: 0.458 |  Val. Acc: 89.67%\n",
      "Epoch: 11 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.168 | Train Acc: 94.96%\n",
      "\t Val. Loss: 0.470 |  Val. Acc: 89.91%\n",
      "Epoch: 12 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.153 | Train Acc: 95.46%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 89.97%\n",
      "Epoch: 13 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.138 | Train Acc: 96.15%\n",
      "\t Val. Loss: 0.516 |  Val. Acc: 89.89%\n",
      "Epoch: 14 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.126 | Train Acc: 96.60%\n",
      "\t Val. Loss: 0.535 |  Val. Acc: 90.17%\n",
      "Epoch: 15 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.114 | Train Acc: 97.10%\n",
      "\t Val. Loss: 0.560 |  Val. Acc: 90.17%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 20,
     "id": "bae0b6e8-3753-4c66-8733-df713ce924d9",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.368 | Test Acc: 86.90%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut3-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "93a41d6a-c955-49f9-a0b1-78dcaf81d9b6",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "## User Input\n",
    "\n",
    "And as before, we can test on any input the user provides making sure to generate bigrams from our tokenized sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 21,
     "id": "1ffae4d0-4028-406a-91c6-b552030fc06c",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = generate_bigrams([tok.text for tok in nlp.tokenizer(sentence)])\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "921802d9-9657-4aeb-9fe4-997b404cdf98",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 22,
     "id": "31a1e34c-c5ab-428f-a25c-81d6788bf404",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3659154839842729e-15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "0b6216df-4523-4c5a-95ae-748eee5e85df",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 23,
     "id": "92abdfcc-2435-4d6f-9e4d-32e30c8a33c9",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 24,
     "id": "1aa98145-2136-4a6b-b456-df7b7356bd00",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ./data\n",
    "!mkdir ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d7ce7ee9-4dfb-4690-95cb-7378d7bf5605",
     "kernelId": "f9eb6953-a139-4a5a-8775-9cbd811b0512"
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook we'll use convolutional neural networks (CNNs) to perform sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
